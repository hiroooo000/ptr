{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "z9YRzBx3XlxE",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 9章　多値分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jklJPHT8XlxM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting japanize-matplotlib\n",
      "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from japanize-matplotlib) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->japanize-matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->japanize-matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->japanize-matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->japanize-matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/conda/lib/python3.11/site-packages (from matplotlib->japanize-matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->japanize-matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->japanize-matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->japanize-matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib->japanize-matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->japanize-matplotlib) (1.16.0)\n",
      "Building wheels for collected packages: japanize-matplotlib\n",
      "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120257 sha256=da4e2caa7e8136aa50263358ee883ce52b9e01b93bc1f8f575deb3ae5db79e80\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/da/a1/71/b8faeb93276fed10edffcca20746f1ef6f8d9e071eee8425fc\n",
      "Successfully built japanize-matplotlib\n",
      "Installing collected packages: japanize-matplotlib\n",
      "Successfully installed japanize-matplotlib-1.1.3\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 日本語化ライブラリ導入\n",
    "!pip install japanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UFF8sgwUXlxN"
   },
   "outputs": [],
   "source": [
    "# 必要ライブラリの宣言\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  Numpy変数print時の小数点表示を2桁に設定\n",
    "np.set_printoptions(\n",
    "    suppress=True, precision=2, floatmode='fixed'\n",
    ")\n",
    "\n",
    "# matplotlib日本語化対応\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEWSofZMXlxN"
   },
   "source": [
    "### データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "di-gkOdyXlxN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元データ (150, 2) (150,)\n"
     ]
    }
   ],
   "source": [
    "# 学習用データ準備\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "x_org, y_org = iris.data, iris.target\n",
    "\n",
    "# 入力データに関しては、sepal length(0)とpetal length(2)のみ抽出\n",
    "x_select = x_org[:,[0,2]]\n",
    "print('元データ', x_select.shape, y_org.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgx2_fUUXlxO"
   },
   "source": [
    "### 学習データの散布図表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R7SWh_qCXlxO"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 散布図の表示\n",
    "x_t0 = x_select[y_org == 0]\n",
    "x_t1 = x_select[y_org == 1]\n",
    "x_t2 = x_select[y_org == 2]\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', c='k', s=50, label='0 (setosa)')\n",
    "plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', c='b', s=50, label='1 (versicolour)')\n",
    "plt.scatter(x_t2[:,0], x_t2[:,1], marker='+', c='k', s=50, label='2 (virginica)')\n",
    "plt.xlabel('sepal_length', fontsize=14)\n",
    "plt.ylabel('petal_length', fontsize=14)\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62M0lLZGXlxP"
   },
   "source": [
    "### データ前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UNnflOuLXlxP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00 5.10 1.40]\n",
      " [1.00 4.90 1.40]\n",
      " [1.00 4.70 1.30]\n",
      " [1.00 4.60 1.50]\n",
      " [1.00 5.00 1.40]]\n"
     ]
    }
   ],
   "source": [
    "# ダミー変数を追加\n",
    "x_all = np.insert(x_select, 0, 1.0, axis=1)\n",
    "print(x_all[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DZnl6EviXlxP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "オリジナル (150,)\n",
      "２次元化 (150, 1)\n",
      "One Hot Vector化後 (150, 3)\n"
     ]
    }
   ],
   "source": [
    "# yをOne-hot-Vectorに\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False,categories='auto')\n",
    "y_work = np.c_[y_org]\n",
    "y_all_one = ohe.fit_transform(y_work)\n",
    "print('オリジナル', y_org.shape)\n",
    "print('２次元化', y_work.shape)\n",
    "print('One Hot Vector化後', y_all_one.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jFxWZtv4XlxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 3) (75, 3) (75,) (75,) (75, 3) (75, 3)\n"
     ]
    }
   ],
   "source": [
    "# 学習データ、検証データに分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_train_one, y_test_one = train_test_split(\n",
    "    x_all, y_org, y_all_one, train_size=75, test_size=75, random_state=123)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape,\n",
    "    y_train_one.shape, y_test_one.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y1-lwDNIXlxQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力データ(x)\n",
      "[[1.00 6.30 4.70]\n",
      " [1.00 7.00 4.70]\n",
      " [1.00 5.00 1.60]\n",
      " [1.00 6.40 5.60]\n",
      " [1.00 6.30 5.00]]\n"
     ]
    }
   ],
   "source": [
    "print('入力データ(x)')\n",
    "print(x_train[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bMOrYsNFXlxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解データ(y)\n",
      "[1 1 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "print('正解データ(y)')\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JcMEZWCzXlxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解データ (One Hot Vector化後)\n",
      "[[0.00 1.00 0.00]\n",
      " [0.00 1.00 0.00]\n",
      " [1.00 0.00 0.00]\n",
      " [0.00 0.00 1.00]\n",
      " [0.00 0.00 1.00]]\n"
     ]
    }
   ],
   "source": [
    "print('正解データ (One Hot Vector化後)')\n",
    "print(y_train_one[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh1lkcVzXlxR"
   },
   "source": [
    "### 学習用変数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_bXIpC79XlxR"
   },
   "outputs": [],
   "source": [
    "# 学習対象の選択\n",
    "x, yt  = x_train, y_train_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QDwnqxDXlxR"
   },
   "source": [
    "### 予測関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ETsZr8TFXlxR"
   },
   "outputs": [],
   "source": [
    "# softmax関数 (9.7.3)\n",
    "def softmax(x):\n",
    "    x = x.T\n",
    "    x_max = x.max(axis=0)\n",
    "    x = x - x_max\n",
    "    w = np.exp(x)\n",
    "    return (w / w.sum(axis=0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uzY1FDAHXlxS"
   },
   "outputs": [],
   "source": [
    "# 予測値の計算 (9.7.1, 9.7.2)\n",
    "def pred(x, W):\n",
    "    return softmax(x @ W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSFkCpUBXlxS"
   },
   "source": [
    "### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dZLDhV3AXlxS"
   },
   "outputs": [],
   "source": [
    "# 交差エントロピー関数)(9.5.1)\n",
    "def cross_entropy(yt, yp):\n",
    "    return -np.mean(np.sum(yt * np.log(yp), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "n1lQvEG4XlxS"
   },
   "outputs": [],
   "source": [
    "# モデルの評価を行う関数\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(x_test, y_test, y_test_one, W):\n",
    "\n",
    "    # 予測値の計算(確率値)\n",
    "    yp_test_one = pred(x_test, W)\n",
    "\n",
    "    # 確率値から予測クラス(0, 1, 2)を導出\n",
    "    yp_test = np.argmax(yp_test_one, axis=1)\n",
    "\n",
    "    # 損失関数値の計算\n",
    "    loss = cross_entropy(y_test_one, yp_test_one)\n",
    "\n",
    "    # 精度の算出\n",
    "    score = accuracy_score(y_test, yp_test)\n",
    "    return loss, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_ffOcGtXlxS"
   },
   "source": [
    "### 初期化処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2KUIOsiVXlxT"
   },
   "outputs": [],
   "source": [
    "# 初期化処理\n",
    "\n",
    "# 標本数\n",
    "M  = x.shape[0]\n",
    "# 入力次元数(ダミー変数を含む\n",
    "D = x.shape[1]\n",
    "# 分類先クラス数\n",
    "N = yt.shape[1]\n",
    "\n",
    "# 繰り返し回数\n",
    "iters = 10000\n",
    "\n",
    "# 学習率\n",
    "alpha = 0.01\n",
    "\n",
    "# 重み行列の初期設定(すべて1)\n",
    "W = np.ones((D, N))\n",
    "\n",
    "# 評価結果記録用\n",
    "history = np.zeros((0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00 1.00 1.00]\n",
      " [1.00 1.00 1.00]\n",
      " [1.00 1.00 1.00]]\n",
      "[[ 7.50  7.50  7.50]\n",
      " [ 7.30  7.30  7.30]\n",
      " [ 7.00  7.00  7.00]\n",
      " [ 7.10  7.10  7.10]\n",
      " [ 7.40  7.40  7.40]\n",
      " [ 8.10  8.10  8.10]\n",
      " [ 7.00  7.00  7.00]\n",
      " [ 7.50  7.50  7.50]\n",
      " [ 6.80  6.80  6.80]\n",
      " [ 7.40  7.40  7.40]\n",
      " [ 7.90  7.90  7.90]\n",
      " [ 7.40  7.40  7.40]\n",
      " [ 7.20  7.20  7.20]\n",
      " [ 6.40  6.40  6.40]\n",
      " [ 8.00  8.00  8.00]\n",
      " [ 8.20  8.20  8.20]\n",
      " [ 7.70  7.70  7.70]\n",
      " [ 7.50  7.50  7.50]\n",
      " [ 8.40  8.40  8.40]\n",
      " [ 7.60  7.60  7.60]\n",
      " [ 8.10  8.10  8.10]\n",
      " [ 7.60  7.60  7.60]\n",
      " [ 6.60  6.60  6.60]\n",
      " [ 7.80  7.80  7.80]\n",
      " [ 7.70  7.70  7.70]\n",
      " [ 7.60  7.60  7.60]\n",
      " [ 7.60  7.60  7.60]\n",
      " [ 7.70  7.70  7.70]\n",
      " [ 7.60  7.60  7.60]\n",
      " [ 7.30  7.30  7.30]\n",
      " [ 7.40  7.40  7.40]\n",
      " [ 7.90  7.90  7.90]\n",
      " [ 7.70  7.70  7.70]\n",
      " [ 7.90  7.90  7.90]\n",
      " [ 7.40  7.40  7.40]\n",
      " [ 7.20  7.20  7.20]\n",
      " [ 7.80  7.80  7.80]\n",
      " [ 7.30  7.30  7.30]\n",
      " [ 6.70  6.70  6.70]\n",
      " [ 7.60  7.60  7.60]\n",
      " [ 7.30  7.30  7.30]\n",
      " [ 6.80  6.80  6.80]\n",
      " [ 6.70  6.70  6.70]\n",
      " [ 7.60  7.60  7.60]\n",
      " [ 8.00  8.00  8.00]\n",
      " [ 7.20  7.20  7.20]\n",
      " [ 7.70  7.70  7.70]\n",
      " [ 7.00  7.00  7.00]\n",
      " [ 7.80  7.80  7.80]\n",
      " [ 7.40  7.40  7.40]\n",
      " [12.70 12.70 12.70]\n",
      " [11.90 11.90 11.90]\n",
      " [12.80 12.80 12.80]\n",
      " [10.50 10.50 10.50]\n",
      " [12.10 12.10 12.10]\n",
      " [11.20 11.20 11.20]\n",
      " [12.00 12.00 12.00]\n",
      " [ 9.20  9.20  9.20]\n",
      " [12.20 12.20 12.20]\n",
      " [10.10 10.10 10.10]\n",
      " [ 9.50  9.50  9.50]\n",
      " [11.10 11.10 11.10]\n",
      " [11.00 11.00 11.00]\n",
      " [11.80 11.80 11.80]\n",
      " [10.20 10.20 10.20]\n",
      " [12.10 12.10 12.10]\n",
      " [11.10 11.10 11.10]\n",
      " [10.90 10.90 10.90]\n",
      " [11.70 11.70 11.70]\n",
      " [10.50 10.50 10.50]\n",
      " [11.70 11.70 11.70]\n",
      " [11.10 11.10 11.10]\n",
      " [12.20 12.20 12.20]\n",
      " [11.80 11.80 11.80]\n",
      " [11.70 11.70 11.70]\n",
      " [12.00 12.00 12.00]\n",
      " [12.60 12.60 12.60]\n",
      " [12.70 12.70 12.70]\n",
      " [11.50 11.50 11.50]\n",
      " [10.20 10.20 10.20]\n",
      " [10.30 10.30 10.30]\n",
      " [10.20 10.20 10.20]\n",
      " [10.70 10.70 10.70]\n",
      " [12.10 12.10 12.10]\n",
      " [10.90 10.90 10.90]\n",
      " [11.50 11.50 11.50]\n",
      " [12.40 12.40 12.40]\n",
      " [11.70 11.70 11.70]\n",
      " [10.70 10.70 10.70]\n",
      " [10.50 10.50 10.50]\n",
      " [10.90 10.90 10.90]\n",
      " [11.70 11.70 11.70]\n",
      " [10.80 10.80 10.80]\n",
      " [ 9.30  9.30  9.30]\n",
      " [10.80 10.80 10.80]\n",
      " [10.90 10.90 10.90]\n",
      " [10.90 10.90 10.90]\n",
      " [11.50 11.50 11.50]\n",
      " [ 9.10  9.10  9.10]\n",
      " [10.80 10.80 10.80]\n",
      " [13.30 13.30 13.30]\n",
      " [11.90 11.90 11.90]\n",
      " [14.00 14.00 14.00]\n",
      " [12.90 12.90 12.90]\n",
      " [13.30 13.30 13.30]\n",
      " [15.20 15.20 15.20]\n",
      " [10.40 10.40 10.40]\n",
      " [14.60 14.60 14.60]\n",
      " [13.50 13.50 13.50]\n",
      " [14.30 14.30 14.30]\n",
      " [12.60 12.60 12.60]\n",
      " [12.70 12.70 12.70]\n",
      " [13.30 13.30 13.30]\n",
      " [11.70 11.70 11.70]\n",
      " [11.90 11.90 11.90]\n",
      " [12.70 12.70 12.70]\n",
      " [13.00 13.00 13.00]\n",
      " [15.40 15.40 15.40]\n",
      " [15.60 15.60 15.60]\n",
      " [12.00 12.00 12.00]\n",
      " [13.60 13.60 13.60]\n",
      " [11.50 11.50 11.50]\n",
      " [15.40 15.40 15.40]\n",
      " [12.20 12.20 12.20]\n",
      " [13.40 13.40 13.40]\n",
      " [14.20 14.20 14.20]\n",
      " [12.00 12.00 12.00]\n",
      " [12.00 12.00 12.00]\n",
      " [13.00 13.00 13.00]\n",
      " [14.00 14.00 14.00]\n",
      " [14.50 14.50 14.50]\n",
      " [15.30 15.30 15.30]\n",
      " [13.00 13.00 13.00]\n",
      " [12.40 12.40 12.40]\n",
      " [12.70 12.70 12.70]\n",
      " [14.80 14.80 14.80]\n",
      " [12.90 12.90 12.90]\n",
      " [12.90 12.90 12.90]\n",
      " [11.80 11.80 11.80]\n",
      " [13.30 13.30 13.30]\n",
      " [13.30 13.30 13.30]\n",
      " [13.00 13.00 13.00]\n",
      " [11.90 11.90 11.90]\n",
      " [13.70 13.70 13.70]\n",
      " [13.40 13.40 13.40]\n",
      " [12.90 12.90 12.90]\n",
      " [12.30 12.30 12.30]\n",
      " [12.70 12.70 12.70]\n",
      " [12.60 12.60 12.60]\n",
      " [12.00 12.00 12.00]]\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(x_all @ W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktMzpuoAXlxT"
   },
   "source": [
    "### メイン処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TQE8ABU7XlxT",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 loss = 1.092628 score = 0.266667\n",
      "epoch = 10 loss = 1.064035 score = 0.266667\n",
      "epoch = 20 loss = 1.033466 score = 0.266667\n",
      "epoch = 30 loss = 1.002645 score = 0.266667\n",
      "epoch = 40 loss = 0.973510 score = 0.266667\n",
      "epoch = 50 loss = 0.946309 score = 0.386667\n",
      "epoch = 60 loss = 0.920978 score = 0.560000\n",
      "epoch = 70 loss = 0.897402 score = 0.600000\n",
      "epoch = 80 loss = 0.875452 score = 0.613333\n",
      "epoch = 90 loss = 0.855005 score = 0.626667\n",
      "epoch = 100 loss = 0.835942 score = 0.626667\n",
      "epoch = 110 loss = 0.818152 score = 0.626667\n",
      "epoch = 120 loss = 0.801532 score = 0.626667\n",
      "epoch = 130 loss = 0.785986 score = 0.626667\n",
      "epoch = 140 loss = 0.771424 score = 0.640000\n",
      "epoch = 150 loss = 0.757767 score = 0.653333\n",
      "epoch = 160 loss = 0.744941 score = 0.680000\n",
      "epoch = 170 loss = 0.732877 score = 0.706667\n",
      "epoch = 180 loss = 0.721513 score = 0.760000\n",
      "epoch = 190 loss = 0.710795 score = 0.786667\n",
      "epoch = 200 loss = 0.700670 score = 0.786667\n",
      "epoch = 210 loss = 0.691092 score = 0.800000\n",
      "epoch = 220 loss = 0.682018 score = 0.800000\n",
      "epoch = 230 loss = 0.673411 score = 0.813333\n",
      "epoch = 240 loss = 0.665236 score = 0.813333\n",
      "epoch = 250 loss = 0.657459 score = 0.826667\n",
      "epoch = 260 loss = 0.650053 score = 0.826667\n",
      "epoch = 270 loss = 0.642990 score = 0.826667\n",
      "epoch = 280 loss = 0.636247 score = 0.826667\n",
      "epoch = 290 loss = 0.629802 score = 0.826667\n",
      "epoch = 300 loss = 0.623633 score = 0.826667\n",
      "epoch = 310 loss = 0.617723 score = 0.826667\n",
      "epoch = 320 loss = 0.612055 score = 0.826667\n",
      "epoch = 330 loss = 0.606612 score = 0.826667\n",
      "epoch = 340 loss = 0.601381 score = 0.840000\n",
      "epoch = 350 loss = 0.596349 score = 0.840000\n",
      "epoch = 360 loss = 0.591502 score = 0.853333\n",
      "epoch = 370 loss = 0.586831 score = 0.866667\n",
      "epoch = 380 loss = 0.582324 score = 0.866667\n",
      "epoch = 390 loss = 0.577971 score = 0.866667\n",
      "epoch = 400 loss = 0.573766 score = 0.866667\n",
      "epoch = 410 loss = 0.569698 score = 0.866667\n",
      "epoch = 420 loss = 0.565760 score = 0.866667\n",
      "epoch = 430 loss = 0.561945 score = 0.866667\n",
      "epoch = 440 loss = 0.558247 score = 0.866667\n",
      "epoch = 450 loss = 0.554660 score = 0.866667\n",
      "epoch = 460 loss = 0.551177 score = 0.866667\n",
      "epoch = 470 loss = 0.547794 score = 0.880000\n",
      "epoch = 480 loss = 0.544505 score = 0.880000\n",
      "epoch = 490 loss = 0.541306 score = 0.880000\n",
      "epoch = 500 loss = 0.538193 score = 0.880000\n",
      "epoch = 510 loss = 0.535161 score = 0.880000\n",
      "epoch = 520 loss = 0.532206 score = 0.880000\n",
      "epoch = 530 loss = 0.529326 score = 0.880000\n",
      "epoch = 540 loss = 0.526516 score = 0.880000\n",
      "epoch = 550 loss = 0.523774 score = 0.880000\n",
      "epoch = 560 loss = 0.521097 score = 0.880000\n",
      "epoch = 570 loss = 0.518481 score = 0.880000\n",
      "epoch = 580 loss = 0.515925 score = 0.880000\n",
      "epoch = 590 loss = 0.513425 score = 0.880000\n",
      "epoch = 600 loss = 0.510979 score = 0.880000\n",
      "epoch = 610 loss = 0.508586 score = 0.880000\n",
      "epoch = 620 loss = 0.506242 score = 0.880000\n",
      "epoch = 630 loss = 0.503947 score = 0.880000\n",
      "epoch = 640 loss = 0.501698 score = 0.880000\n",
      "epoch = 650 loss = 0.499493 score = 0.880000\n",
      "epoch = 660 loss = 0.497332 score = 0.893333\n",
      "epoch = 670 loss = 0.495211 score = 0.906667\n",
      "epoch = 680 loss = 0.493130 score = 0.906667\n",
      "epoch = 690 loss = 0.491087 score = 0.906667\n",
      "epoch = 700 loss = 0.489081 score = 0.906667\n",
      "epoch = 710 loss = 0.487111 score = 0.906667\n",
      "epoch = 720 loss = 0.485175 score = 0.906667\n",
      "epoch = 730 loss = 0.483272 score = 0.906667\n",
      "epoch = 740 loss = 0.481401 score = 0.906667\n",
      "epoch = 750 loss = 0.479562 score = 0.906667\n",
      "epoch = 760 loss = 0.477752 score = 0.906667\n",
      "epoch = 770 loss = 0.475971 score = 0.906667\n",
      "epoch = 780 loss = 0.474219 score = 0.906667\n",
      "epoch = 790 loss = 0.472494 score = 0.920000\n",
      "epoch = 800 loss = 0.470795 score = 0.920000\n",
      "epoch = 810 loss = 0.469121 score = 0.920000\n",
      "epoch = 820 loss = 0.467473 score = 0.920000\n",
      "epoch = 830 loss = 0.465848 score = 0.920000\n",
      "epoch = 840 loss = 0.464247 score = 0.920000\n",
      "epoch = 850 loss = 0.462668 score = 0.920000\n",
      "epoch = 860 loss = 0.461112 score = 0.920000\n",
      "epoch = 870 loss = 0.459577 score = 0.920000\n",
      "epoch = 880 loss = 0.458063 score = 0.920000\n",
      "epoch = 890 loss = 0.456569 score = 0.920000\n",
      "epoch = 900 loss = 0.455095 score = 0.920000\n",
      "epoch = 910 loss = 0.453639 score = 0.920000\n",
      "epoch = 920 loss = 0.452203 score = 0.920000\n",
      "epoch = 930 loss = 0.450785 score = 0.920000\n",
      "epoch = 940 loss = 0.449384 score = 0.920000\n",
      "epoch = 950 loss = 0.448001 score = 0.920000\n",
      "epoch = 960 loss = 0.446634 score = 0.920000\n",
      "epoch = 970 loss = 0.445284 score = 0.920000\n",
      "epoch = 980 loss = 0.443950 score = 0.920000\n",
      "epoch = 990 loss = 0.442632 score = 0.920000\n",
      "epoch = 1000 loss = 0.441329 score = 0.920000\n",
      "epoch = 1010 loss = 0.440040 score = 0.920000\n",
      "epoch = 1020 loss = 0.438766 score = 0.920000\n",
      "epoch = 1030 loss = 0.437507 score = 0.920000\n",
      "epoch = 1040 loss = 0.436261 score = 0.920000\n",
      "epoch = 1050 loss = 0.435029 score = 0.920000\n",
      "epoch = 1060 loss = 0.433810 score = 0.920000\n",
      "epoch = 1070 loss = 0.432604 score = 0.920000\n",
      "epoch = 1080 loss = 0.431410 score = 0.920000\n",
      "epoch = 1090 loss = 0.430230 score = 0.920000\n",
      "epoch = 1100 loss = 0.429061 score = 0.920000\n",
      "epoch = 1110 loss = 0.427904 score = 0.920000\n",
      "epoch = 1120 loss = 0.426759 score = 0.920000\n",
      "epoch = 1130 loss = 0.425625 score = 0.920000\n",
      "epoch = 1140 loss = 0.424502 score = 0.920000\n",
      "epoch = 1150 loss = 0.423390 score = 0.920000\n",
      "epoch = 1160 loss = 0.422289 score = 0.920000\n",
      "epoch = 1170 loss = 0.421199 score = 0.920000\n",
      "epoch = 1180 loss = 0.420119 score = 0.920000\n",
      "epoch = 1190 loss = 0.419048 score = 0.920000\n",
      "epoch = 1200 loss = 0.417988 score = 0.920000\n",
      "epoch = 1210 loss = 0.416938 score = 0.920000\n",
      "epoch = 1220 loss = 0.415897 score = 0.920000\n",
      "epoch = 1230 loss = 0.414865 score = 0.920000\n",
      "epoch = 1240 loss = 0.413843 score = 0.920000\n",
      "epoch = 1250 loss = 0.412830 score = 0.920000\n",
      "epoch = 1260 loss = 0.411825 score = 0.920000\n",
      "epoch = 1270 loss = 0.410830 score = 0.920000\n",
      "epoch = 1280 loss = 0.409843 score = 0.920000\n",
      "epoch = 1290 loss = 0.408864 score = 0.920000\n",
      "epoch = 1300 loss = 0.407893 score = 0.920000\n",
      "epoch = 1310 loss = 0.406931 score = 0.920000\n",
      "epoch = 1320 loss = 0.405977 score = 0.920000\n",
      "epoch = 1330 loss = 0.405030 score = 0.933333\n",
      "epoch = 1340 loss = 0.404092 score = 0.933333\n",
      "epoch = 1350 loss = 0.403161 score = 0.933333\n",
      "epoch = 1360 loss = 0.402237 score = 0.933333\n",
      "epoch = 1370 loss = 0.401321 score = 0.933333\n",
      "epoch = 1380 loss = 0.400412 score = 0.933333\n",
      "epoch = 1390 loss = 0.399510 score = 0.933333\n",
      "epoch = 1400 loss = 0.398615 score = 0.933333\n",
      "epoch = 1410 loss = 0.397727 score = 0.933333\n",
      "epoch = 1420 loss = 0.396846 score = 0.933333\n",
      "epoch = 1430 loss = 0.395971 score = 0.933333\n",
      "epoch = 1440 loss = 0.395104 score = 0.933333\n",
      "epoch = 1450 loss = 0.394242 score = 0.933333\n",
      "epoch = 1460 loss = 0.393387 score = 0.933333\n",
      "epoch = 1470 loss = 0.392539 score = 0.933333\n",
      "epoch = 1480 loss = 0.391697 score = 0.933333\n",
      "epoch = 1490 loss = 0.390860 score = 0.933333\n",
      "epoch = 1500 loss = 0.390030 score = 0.933333\n",
      "epoch = 1510 loss = 0.389206 score = 0.933333\n",
      "epoch = 1520 loss = 0.388388 score = 0.933333\n",
      "epoch = 1530 loss = 0.387575 score = 0.933333\n",
      "epoch = 1540 loss = 0.386769 score = 0.933333\n",
      "epoch = 1550 loss = 0.385968 score = 0.933333\n",
      "epoch = 1560 loss = 0.385172 score = 0.946667\n",
      "epoch = 1570 loss = 0.384382 score = 0.946667\n",
      "epoch = 1580 loss = 0.383598 score = 0.946667\n",
      "epoch = 1590 loss = 0.382818 score = 0.946667\n",
      "epoch = 1600 loss = 0.382044 score = 0.946667\n",
      "epoch = 1610 loss = 0.381276 score = 0.946667\n",
      "epoch = 1620 loss = 0.380512 score = 0.946667\n",
      "epoch = 1630 loss = 0.379754 score = 0.946667\n",
      "epoch = 1640 loss = 0.379000 score = 0.946667\n",
      "epoch = 1650 loss = 0.378252 score = 0.946667\n",
      "epoch = 1660 loss = 0.377508 score = 0.946667\n",
      "epoch = 1670 loss = 0.376769 score = 0.946667\n",
      "epoch = 1680 loss = 0.376035 score = 0.946667\n",
      "epoch = 1690 loss = 0.375306 score = 0.946667\n",
      "epoch = 1700 loss = 0.374582 score = 0.946667\n",
      "epoch = 1710 loss = 0.373862 score = 0.946667\n",
      "epoch = 1720 loss = 0.373146 score = 0.946667\n",
      "epoch = 1730 loss = 0.372435 score = 0.946667\n",
      "epoch = 1740 loss = 0.371729 score = 0.946667\n",
      "epoch = 1750 loss = 0.371027 score = 0.946667\n",
      "epoch = 1760 loss = 0.370329 score = 0.946667\n",
      "epoch = 1770 loss = 0.369636 score = 0.946667\n",
      "epoch = 1780 loss = 0.368947 score = 0.946667\n",
      "epoch = 1790 loss = 0.368262 score = 0.946667\n",
      "epoch = 1800 loss = 0.367581 score = 0.946667\n",
      "epoch = 1810 loss = 0.366904 score = 0.946667\n",
      "epoch = 1820 loss = 0.366232 score = 0.946667\n",
      "epoch = 1830 loss = 0.365563 score = 0.946667\n",
      "epoch = 1840 loss = 0.364899 score = 0.946667\n",
      "epoch = 1850 loss = 0.364238 score = 0.946667\n",
      "epoch = 1860 loss = 0.363581 score = 0.946667\n",
      "epoch = 1870 loss = 0.362929 score = 0.946667\n",
      "epoch = 1880 loss = 0.362280 score = 0.946667\n",
      "epoch = 1890 loss = 0.361634 score = 0.946667\n",
      "epoch = 1900 loss = 0.360993 score = 0.946667\n",
      "epoch = 1910 loss = 0.360355 score = 0.946667\n",
      "epoch = 1920 loss = 0.359721 score = 0.946667\n",
      "epoch = 1930 loss = 0.359090 score = 0.946667\n",
      "epoch = 1940 loss = 0.358463 score = 0.946667\n",
      "epoch = 1950 loss = 0.357840 score = 0.946667\n",
      "epoch = 1960 loss = 0.357220 score = 0.946667\n",
      "epoch = 1970 loss = 0.356603 score = 0.946667\n",
      "epoch = 1980 loss = 0.355990 score = 0.946667\n",
      "epoch = 1990 loss = 0.355381 score = 0.946667\n",
      "epoch = 2000 loss = 0.354775 score = 0.946667\n",
      "epoch = 2010 loss = 0.354172 score = 0.946667\n",
      "epoch = 2020 loss = 0.353572 score = 0.946667\n",
      "epoch = 2030 loss = 0.352976 score = 0.946667\n",
      "epoch = 2040 loss = 0.352383 score = 0.946667\n",
      "epoch = 2050 loss = 0.351793 score = 0.946667\n",
      "epoch = 2060 loss = 0.351207 score = 0.946667\n",
      "epoch = 2070 loss = 0.350623 score = 0.946667\n",
      "epoch = 2080 loss = 0.350043 score = 0.946667\n",
      "epoch = 2090 loss = 0.349466 score = 0.946667\n",
      "epoch = 2100 loss = 0.348892 score = 0.946667\n",
      "epoch = 2110 loss = 0.348321 score = 0.946667\n",
      "epoch = 2120 loss = 0.347753 score = 0.946667\n",
      "epoch = 2130 loss = 0.347188 score = 0.946667\n",
      "epoch = 2140 loss = 0.346626 score = 0.946667\n",
      "epoch = 2150 loss = 0.346066 score = 0.946667\n",
      "epoch = 2160 loss = 0.345510 score = 0.946667\n",
      "epoch = 2170 loss = 0.344957 score = 0.946667\n",
      "epoch = 2180 loss = 0.344406 score = 0.946667\n",
      "epoch = 2190 loss = 0.343859 score = 0.946667\n",
      "epoch = 2200 loss = 0.343314 score = 0.946667\n",
      "epoch = 2210 loss = 0.342772 score = 0.946667\n",
      "epoch = 2220 loss = 0.342233 score = 0.946667\n",
      "epoch = 2230 loss = 0.341696 score = 0.946667\n",
      "epoch = 2240 loss = 0.341162 score = 0.946667\n",
      "epoch = 2250 loss = 0.340631 score = 0.946667\n",
      "epoch = 2260 loss = 0.340103 score = 0.946667\n",
      "epoch = 2270 loss = 0.339577 score = 0.946667\n",
      "epoch = 2280 loss = 0.339054 score = 0.946667\n",
      "epoch = 2290 loss = 0.338533 score = 0.946667\n",
      "epoch = 2300 loss = 0.338015 score = 0.946667\n",
      "epoch = 2310 loss = 0.337500 score = 0.946667\n",
      "epoch = 2320 loss = 0.336987 score = 0.946667\n",
      "epoch = 2330 loss = 0.336477 score = 0.946667\n",
      "epoch = 2340 loss = 0.335969 score = 0.946667\n",
      "epoch = 2350 loss = 0.335463 score = 0.946667\n",
      "epoch = 2360 loss = 0.334960 score = 0.946667\n",
      "epoch = 2370 loss = 0.334460 score = 0.946667\n",
      "epoch = 2380 loss = 0.333962 score = 0.946667\n",
      "epoch = 2390 loss = 0.333466 score = 0.946667\n",
      "epoch = 2400 loss = 0.332973 score = 0.946667\n",
      "epoch = 2410 loss = 0.332482 score = 0.946667\n",
      "epoch = 2420 loss = 0.331994 score = 0.946667\n",
      "epoch = 2430 loss = 0.331507 score = 0.946667\n",
      "epoch = 2440 loss = 0.331023 score = 0.946667\n",
      "epoch = 2450 loss = 0.330542 score = 0.946667\n",
      "epoch = 2460 loss = 0.330063 score = 0.946667\n",
      "epoch = 2470 loss = 0.329585 score = 0.946667\n",
      "epoch = 2480 loss = 0.329111 score = 0.946667\n",
      "epoch = 2490 loss = 0.328638 score = 0.946667\n",
      "epoch = 2500 loss = 0.328168 score = 0.946667\n",
      "epoch = 2510 loss = 0.327699 score = 0.946667\n",
      "epoch = 2520 loss = 0.327233 score = 0.946667\n",
      "epoch = 2530 loss = 0.326769 score = 0.946667\n",
      "epoch = 2540 loss = 0.326308 score = 0.946667\n",
      "epoch = 2550 loss = 0.325848 score = 0.946667\n",
      "epoch = 2560 loss = 0.325390 score = 0.946667\n",
      "epoch = 2570 loss = 0.324935 score = 0.946667\n",
      "epoch = 2580 loss = 0.324482 score = 0.946667\n",
      "epoch = 2590 loss = 0.324030 score = 0.946667\n",
      "epoch = 2600 loss = 0.323581 score = 0.946667\n",
      "epoch = 2610 loss = 0.323134 score = 0.946667\n",
      "epoch = 2620 loss = 0.322689 score = 0.946667\n",
      "epoch = 2630 loss = 0.322245 score = 0.946667\n",
      "epoch = 2640 loss = 0.321804 score = 0.946667\n",
      "epoch = 2650 loss = 0.321365 score = 0.946667\n",
      "epoch = 2660 loss = 0.320928 score = 0.946667\n",
      "epoch = 2670 loss = 0.320492 score = 0.946667\n",
      "epoch = 2680 loss = 0.320059 score = 0.946667\n",
      "epoch = 2690 loss = 0.319628 score = 0.946667\n",
      "epoch = 2700 loss = 0.319198 score = 0.946667\n",
      "epoch = 2710 loss = 0.318770 score = 0.946667\n",
      "epoch = 2720 loss = 0.318345 score = 0.946667\n",
      "epoch = 2730 loss = 0.317921 score = 0.946667\n",
      "epoch = 2740 loss = 0.317499 score = 0.946667\n",
      "epoch = 2750 loss = 0.317078 score = 0.946667\n",
      "epoch = 2760 loss = 0.316660 score = 0.946667\n",
      "epoch = 2770 loss = 0.316243 score = 0.946667\n",
      "epoch = 2780 loss = 0.315829 score = 0.946667\n",
      "epoch = 2790 loss = 0.315416 score = 0.946667\n",
      "epoch = 2800 loss = 0.315004 score = 0.946667\n",
      "epoch = 2810 loss = 0.314595 score = 0.946667\n",
      "epoch = 2820 loss = 0.314187 score = 0.946667\n",
      "epoch = 2830 loss = 0.313781 score = 0.946667\n",
      "epoch = 2840 loss = 0.313377 score = 0.946667\n",
      "epoch = 2850 loss = 0.312975 score = 0.946667\n",
      "epoch = 2860 loss = 0.312574 score = 0.946667\n",
      "epoch = 2870 loss = 0.312175 score = 0.946667\n",
      "epoch = 2880 loss = 0.311777 score = 0.946667\n",
      "epoch = 2890 loss = 0.311382 score = 0.946667\n",
      "epoch = 2900 loss = 0.310988 score = 0.946667\n",
      "epoch = 2910 loss = 0.310595 score = 0.946667\n",
      "epoch = 2920 loss = 0.310204 score = 0.946667\n",
      "epoch = 2930 loss = 0.309815 score = 0.946667\n",
      "epoch = 2940 loss = 0.309428 score = 0.946667\n",
      "epoch = 2950 loss = 0.309042 score = 0.946667\n",
      "epoch = 2960 loss = 0.308658 score = 0.946667\n",
      "epoch = 2970 loss = 0.308275 score = 0.946667\n",
      "epoch = 2980 loss = 0.307894 score = 0.946667\n",
      "epoch = 2990 loss = 0.307514 score = 0.946667\n",
      "epoch = 3000 loss = 0.307136 score = 0.946667\n",
      "epoch = 3010 loss = 0.306760 score = 0.946667\n",
      "epoch = 3020 loss = 0.306385 score = 0.946667\n",
      "epoch = 3030 loss = 0.306011 score = 0.946667\n",
      "epoch = 3040 loss = 0.305639 score = 0.946667\n",
      "epoch = 3050 loss = 0.305269 score = 0.946667\n",
      "epoch = 3060 loss = 0.304900 score = 0.946667\n",
      "epoch = 3070 loss = 0.304533 score = 0.946667\n",
      "epoch = 3080 loss = 0.304167 score = 0.946667\n",
      "epoch = 3090 loss = 0.303803 score = 0.946667\n",
      "epoch = 3100 loss = 0.303440 score = 0.946667\n",
      "epoch = 3110 loss = 0.303078 score = 0.946667\n",
      "epoch = 3120 loss = 0.302718 score = 0.946667\n",
      "epoch = 3130 loss = 0.302359 score = 0.946667\n",
      "epoch = 3140 loss = 0.302002 score = 0.946667\n",
      "epoch = 3150 loss = 0.301646 score = 0.946667\n",
      "epoch = 3160 loss = 0.301292 score = 0.946667\n",
      "epoch = 3170 loss = 0.300939 score = 0.946667\n",
      "epoch = 3180 loss = 0.300588 score = 0.946667\n",
      "epoch = 3190 loss = 0.300237 score = 0.946667\n",
      "epoch = 3200 loss = 0.299889 score = 0.946667\n",
      "epoch = 3210 loss = 0.299541 score = 0.946667\n",
      "epoch = 3220 loss = 0.299195 score = 0.946667\n",
      "epoch = 3230 loss = 0.298850 score = 0.946667\n",
      "epoch = 3240 loss = 0.298507 score = 0.946667\n",
      "epoch = 3250 loss = 0.298165 score = 0.946667\n",
      "epoch = 3260 loss = 0.297824 score = 0.946667\n",
      "epoch = 3270 loss = 0.297485 score = 0.946667\n",
      "epoch = 3280 loss = 0.297147 score = 0.946667\n",
      "epoch = 3290 loss = 0.296810 score = 0.946667\n",
      "epoch = 3300 loss = 0.296474 score = 0.946667\n",
      "epoch = 3310 loss = 0.296140 score = 0.946667\n",
      "epoch = 3320 loss = 0.295807 score = 0.946667\n",
      "epoch = 3330 loss = 0.295476 score = 0.946667\n",
      "epoch = 3340 loss = 0.295145 score = 0.946667\n",
      "epoch = 3350 loss = 0.294816 score = 0.946667\n",
      "epoch = 3360 loss = 0.294488 score = 0.946667\n",
      "epoch = 3370 loss = 0.294162 score = 0.946667\n",
      "epoch = 3380 loss = 0.293836 score = 0.946667\n",
      "epoch = 3390 loss = 0.293512 score = 0.946667\n",
      "epoch = 3400 loss = 0.293189 score = 0.946667\n",
      "epoch = 3410 loss = 0.292868 score = 0.946667\n",
      "epoch = 3420 loss = 0.292547 score = 0.946667\n",
      "epoch = 3430 loss = 0.292228 score = 0.946667\n",
      "epoch = 3440 loss = 0.291910 score = 0.946667\n",
      "epoch = 3450 loss = 0.291593 score = 0.946667\n",
      "epoch = 3460 loss = 0.291277 score = 0.946667\n",
      "epoch = 3470 loss = 0.290962 score = 0.946667\n",
      "epoch = 3480 loss = 0.290649 score = 0.946667\n",
      "epoch = 3490 loss = 0.290337 score = 0.946667\n",
      "epoch = 3500 loss = 0.290026 score = 0.946667\n",
      "epoch = 3510 loss = 0.289716 score = 0.946667\n",
      "epoch = 3520 loss = 0.289407 score = 0.946667\n",
      "epoch = 3530 loss = 0.289099 score = 0.946667\n",
      "epoch = 3540 loss = 0.288793 score = 0.946667\n",
      "epoch = 3550 loss = 0.288487 score = 0.946667\n",
      "epoch = 3560 loss = 0.288183 score = 0.946667\n",
      "epoch = 3570 loss = 0.287880 score = 0.946667\n",
      "epoch = 3580 loss = 0.287578 score = 0.946667\n",
      "epoch = 3590 loss = 0.287277 score = 0.946667\n",
      "epoch = 3600 loss = 0.286977 score = 0.946667\n",
      "epoch = 3610 loss = 0.286678 score = 0.946667\n",
      "epoch = 3620 loss = 0.286380 score = 0.960000\n",
      "epoch = 3630 loss = 0.286084 score = 0.960000\n",
      "epoch = 3640 loss = 0.285788 score = 0.960000\n",
      "epoch = 3650 loss = 0.285494 score = 0.960000\n",
      "epoch = 3660 loss = 0.285200 score = 0.960000\n",
      "epoch = 3670 loss = 0.284908 score = 0.960000\n",
      "epoch = 3680 loss = 0.284616 score = 0.960000\n",
      "epoch = 3690 loss = 0.284326 score = 0.960000\n",
      "epoch = 3700 loss = 0.284037 score = 0.960000\n",
      "epoch = 3710 loss = 0.283749 score = 0.960000\n",
      "epoch = 3720 loss = 0.283461 score = 0.960000\n",
      "epoch = 3730 loss = 0.283175 score = 0.960000\n",
      "epoch = 3740 loss = 0.282890 score = 0.960000\n",
      "epoch = 3750 loss = 0.282606 score = 0.960000\n",
      "epoch = 3760 loss = 0.282322 score = 0.960000\n",
      "epoch = 3770 loss = 0.282040 score = 0.960000\n",
      "epoch = 3780 loss = 0.281759 score = 0.960000\n",
      "epoch = 3790 loss = 0.281479 score = 0.960000\n",
      "epoch = 3800 loss = 0.281200 score = 0.960000\n",
      "epoch = 3810 loss = 0.280921 score = 0.960000\n",
      "epoch = 3820 loss = 0.280644 score = 0.960000\n",
      "epoch = 3830 loss = 0.280368 score = 0.960000\n",
      "epoch = 3840 loss = 0.280092 score = 0.960000\n",
      "epoch = 3850 loss = 0.279818 score = 0.960000\n",
      "epoch = 3860 loss = 0.279544 score = 0.960000\n",
      "epoch = 3870 loss = 0.279272 score = 0.960000\n",
      "epoch = 3880 loss = 0.279000 score = 0.960000\n",
      "epoch = 3890 loss = 0.278729 score = 0.960000\n",
      "epoch = 3900 loss = 0.278460 score = 0.960000\n",
      "epoch = 3910 loss = 0.278191 score = 0.960000\n",
      "epoch = 3920 loss = 0.277923 score = 0.960000\n",
      "epoch = 3930 loss = 0.277656 score = 0.960000\n",
      "epoch = 3940 loss = 0.277390 score = 0.960000\n",
      "epoch = 3950 loss = 0.277125 score = 0.960000\n",
      "epoch = 3960 loss = 0.276861 score = 0.960000\n",
      "epoch = 3970 loss = 0.276597 score = 0.960000\n",
      "epoch = 3980 loss = 0.276335 score = 0.960000\n",
      "epoch = 3990 loss = 0.276073 score = 0.960000\n",
      "epoch = 4000 loss = 0.275812 score = 0.960000\n",
      "epoch = 4010 loss = 0.275553 score = 0.960000\n",
      "epoch = 4020 loss = 0.275294 score = 0.960000\n",
      "epoch = 4030 loss = 0.275036 score = 0.960000\n",
      "epoch = 4040 loss = 0.274778 score = 0.960000\n",
      "epoch = 4050 loss = 0.274522 score = 0.960000\n",
      "epoch = 4060 loss = 0.274266 score = 0.960000\n",
      "epoch = 4070 loss = 0.274012 score = 0.960000\n",
      "epoch = 4080 loss = 0.273758 score = 0.960000\n",
      "epoch = 4090 loss = 0.273505 score = 0.960000\n",
      "epoch = 4100 loss = 0.273253 score = 0.960000\n",
      "epoch = 4110 loss = 0.273002 score = 0.960000\n",
      "epoch = 4120 loss = 0.272751 score = 0.960000\n",
      "epoch = 4130 loss = 0.272502 score = 0.960000\n",
      "epoch = 4140 loss = 0.272253 score = 0.960000\n",
      "epoch = 4150 loss = 0.272005 score = 0.960000\n",
      "epoch = 4160 loss = 0.271758 score = 0.960000\n",
      "epoch = 4170 loss = 0.271511 score = 0.960000\n",
      "epoch = 4180 loss = 0.271266 score = 0.960000\n",
      "epoch = 4190 loss = 0.271021 score = 0.960000\n",
      "epoch = 4200 loss = 0.270777 score = 0.960000\n",
      "epoch = 4210 loss = 0.270534 score = 0.960000\n",
      "epoch = 4220 loss = 0.270291 score = 0.960000\n",
      "epoch = 4230 loss = 0.270050 score = 0.960000\n",
      "epoch = 4240 loss = 0.269809 score = 0.960000\n",
      "epoch = 4250 loss = 0.269569 score = 0.960000\n",
      "epoch = 4260 loss = 0.269330 score = 0.960000\n",
      "epoch = 4270 loss = 0.269091 score = 0.960000\n",
      "epoch = 4280 loss = 0.268854 score = 0.960000\n",
      "epoch = 4290 loss = 0.268617 score = 0.960000\n",
      "epoch = 4300 loss = 0.268380 score = 0.960000\n",
      "epoch = 4310 loss = 0.268145 score = 0.960000\n",
      "epoch = 4320 loss = 0.267910 score = 0.960000\n",
      "epoch = 4330 loss = 0.267676 score = 0.960000\n",
      "epoch = 4340 loss = 0.267443 score = 0.960000\n",
      "epoch = 4350 loss = 0.267211 score = 0.960000\n",
      "epoch = 4360 loss = 0.266979 score = 0.960000\n",
      "epoch = 4370 loss = 0.266748 score = 0.960000\n",
      "epoch = 4380 loss = 0.266518 score = 0.960000\n",
      "epoch = 4390 loss = 0.266288 score = 0.960000\n",
      "epoch = 4400 loss = 0.266060 score = 0.960000\n",
      "epoch = 4410 loss = 0.265831 score = 0.960000\n",
      "epoch = 4420 loss = 0.265604 score = 0.960000\n",
      "epoch = 4430 loss = 0.265378 score = 0.960000\n",
      "epoch = 4440 loss = 0.265152 score = 0.960000\n",
      "epoch = 4450 loss = 0.264926 score = 0.960000\n",
      "epoch = 4460 loss = 0.264702 score = 0.960000\n",
      "epoch = 4470 loss = 0.264478 score = 0.960000\n",
      "epoch = 4480 loss = 0.264255 score = 0.960000\n",
      "epoch = 4490 loss = 0.264033 score = 0.960000\n",
      "epoch = 4500 loss = 0.263811 score = 0.960000\n",
      "epoch = 4510 loss = 0.263590 score = 0.960000\n",
      "epoch = 4520 loss = 0.263369 score = 0.960000\n",
      "epoch = 4530 loss = 0.263150 score = 0.960000\n",
      "epoch = 4540 loss = 0.262931 score = 0.960000\n",
      "epoch = 4550 loss = 0.262712 score = 0.960000\n",
      "epoch = 4560 loss = 0.262495 score = 0.960000\n",
      "epoch = 4570 loss = 0.262278 score = 0.960000\n",
      "epoch = 4580 loss = 0.262061 score = 0.960000\n",
      "epoch = 4590 loss = 0.261846 score = 0.960000\n",
      "epoch = 4600 loss = 0.261631 score = 0.960000\n",
      "epoch = 4610 loss = 0.261416 score = 0.960000\n",
      "epoch = 4620 loss = 0.261203 score = 0.960000\n",
      "epoch = 4630 loss = 0.260990 score = 0.960000\n",
      "epoch = 4640 loss = 0.260777 score = 0.960000\n",
      "epoch = 4650 loss = 0.260566 score = 0.960000\n",
      "epoch = 4660 loss = 0.260354 score = 0.960000\n",
      "epoch = 4670 loss = 0.260144 score = 0.960000\n",
      "epoch = 4680 loss = 0.259934 score = 0.960000\n",
      "epoch = 4690 loss = 0.259725 score = 0.960000\n",
      "epoch = 4700 loss = 0.259516 score = 0.960000\n",
      "epoch = 4710 loss = 0.259308 score = 0.960000\n",
      "epoch = 4720 loss = 0.259101 score = 0.960000\n",
      "epoch = 4730 loss = 0.258894 score = 0.960000\n",
      "epoch = 4740 loss = 0.258688 score = 0.960000\n",
      "epoch = 4750 loss = 0.258483 score = 0.960000\n",
      "epoch = 4760 loss = 0.258278 score = 0.960000\n",
      "epoch = 4770 loss = 0.258074 score = 0.960000\n",
      "epoch = 4780 loss = 0.257870 score = 0.960000\n",
      "epoch = 4790 loss = 0.257667 score = 0.960000\n",
      "epoch = 4800 loss = 0.257465 score = 0.960000\n",
      "epoch = 4810 loss = 0.257263 score = 0.960000\n",
      "epoch = 4820 loss = 0.257062 score = 0.960000\n",
      "epoch = 4830 loss = 0.256861 score = 0.960000\n",
      "epoch = 4840 loss = 0.256661 score = 0.960000\n",
      "epoch = 4850 loss = 0.256462 score = 0.960000\n",
      "epoch = 4860 loss = 0.256263 score = 0.960000\n",
      "epoch = 4870 loss = 0.256064 score = 0.960000\n",
      "epoch = 4880 loss = 0.255867 score = 0.960000\n",
      "epoch = 4890 loss = 0.255670 score = 0.960000\n",
      "epoch = 4900 loss = 0.255473 score = 0.960000\n",
      "epoch = 4910 loss = 0.255277 score = 0.960000\n",
      "epoch = 4920 loss = 0.255082 score = 0.960000\n",
      "epoch = 4930 loss = 0.254887 score = 0.960000\n",
      "epoch = 4940 loss = 0.254692 score = 0.960000\n",
      "epoch = 4950 loss = 0.254499 score = 0.960000\n",
      "epoch = 4960 loss = 0.254306 score = 0.960000\n",
      "epoch = 4970 loss = 0.254113 score = 0.960000\n",
      "epoch = 4980 loss = 0.253921 score = 0.960000\n",
      "epoch = 4990 loss = 0.253730 score = 0.960000\n",
      "epoch = 5000 loss = 0.253539 score = 0.960000\n",
      "epoch = 5010 loss = 0.253348 score = 0.960000\n",
      "epoch = 5020 loss = 0.253158 score = 0.960000\n",
      "epoch = 5030 loss = 0.252969 score = 0.960000\n",
      "epoch = 5040 loss = 0.252780 score = 0.960000\n",
      "epoch = 5050 loss = 0.252592 score = 0.960000\n",
      "epoch = 5060 loss = 0.252404 score = 0.960000\n",
      "epoch = 5070 loss = 0.252217 score = 0.960000\n",
      "epoch = 5080 loss = 0.252031 score = 0.960000\n",
      "epoch = 5090 loss = 0.251845 score = 0.960000\n",
      "epoch = 5100 loss = 0.251659 score = 0.960000\n",
      "epoch = 5110 loss = 0.251474 score = 0.960000\n",
      "epoch = 5120 loss = 0.251289 score = 0.960000\n",
      "epoch = 5130 loss = 0.251105 score = 0.960000\n",
      "epoch = 5140 loss = 0.250922 score = 0.960000\n",
      "epoch = 5150 loss = 0.250739 score = 0.960000\n",
      "epoch = 5160 loss = 0.250557 score = 0.960000\n",
      "epoch = 5170 loss = 0.250375 score = 0.960000\n",
      "epoch = 5180 loss = 0.250193 score = 0.960000\n",
      "epoch = 5190 loss = 0.250012 score = 0.960000\n",
      "epoch = 5200 loss = 0.249832 score = 0.960000\n",
      "epoch = 5210 loss = 0.249652 score = 0.960000\n",
      "epoch = 5220 loss = 0.249473 score = 0.960000\n",
      "epoch = 5230 loss = 0.249294 score = 0.960000\n",
      "epoch = 5240 loss = 0.249115 score = 0.960000\n",
      "epoch = 5250 loss = 0.248937 score = 0.960000\n",
      "epoch = 5260 loss = 0.248760 score = 0.960000\n",
      "epoch = 5270 loss = 0.248583 score = 0.960000\n",
      "epoch = 5280 loss = 0.248407 score = 0.960000\n",
      "epoch = 5290 loss = 0.248231 score = 0.960000\n",
      "epoch = 5300 loss = 0.248055 score = 0.960000\n",
      "epoch = 5310 loss = 0.247880 score = 0.960000\n",
      "epoch = 5320 loss = 0.247706 score = 0.960000\n",
      "epoch = 5330 loss = 0.247532 score = 0.960000\n",
      "epoch = 5340 loss = 0.247358 score = 0.960000\n",
      "epoch = 5350 loss = 0.247185 score = 0.960000\n",
      "epoch = 5360 loss = 0.247013 score = 0.960000\n",
      "epoch = 5370 loss = 0.246840 score = 0.960000\n",
      "epoch = 5380 loss = 0.246669 score = 0.960000\n",
      "epoch = 5390 loss = 0.246498 score = 0.960000\n",
      "epoch = 5400 loss = 0.246327 score = 0.960000\n",
      "epoch = 5410 loss = 0.246157 score = 0.960000\n",
      "epoch = 5420 loss = 0.245987 score = 0.960000\n",
      "epoch = 5430 loss = 0.245817 score = 0.960000\n",
      "epoch = 5440 loss = 0.245649 score = 0.960000\n",
      "epoch = 5450 loss = 0.245480 score = 0.960000\n",
      "epoch = 5460 loss = 0.245312 score = 0.960000\n",
      "epoch = 5470 loss = 0.245145 score = 0.960000\n",
      "epoch = 5480 loss = 0.244978 score = 0.960000\n",
      "epoch = 5490 loss = 0.244811 score = 0.960000\n",
      "epoch = 5500 loss = 0.244645 score = 0.960000\n",
      "epoch = 5510 loss = 0.244479 score = 0.960000\n",
      "epoch = 5520 loss = 0.244314 score = 0.960000\n",
      "epoch = 5530 loss = 0.244149 score = 0.960000\n",
      "epoch = 5540 loss = 0.243984 score = 0.960000\n",
      "epoch = 5550 loss = 0.243821 score = 0.960000\n",
      "epoch = 5560 loss = 0.243657 score = 0.960000\n",
      "epoch = 5570 loss = 0.243494 score = 0.960000\n",
      "epoch = 5580 loss = 0.243331 score = 0.960000\n",
      "epoch = 5590 loss = 0.243169 score = 0.960000\n",
      "epoch = 5600 loss = 0.243007 score = 0.960000\n",
      "epoch = 5610 loss = 0.242846 score = 0.960000\n",
      "epoch = 5620 loss = 0.242685 score = 0.960000\n",
      "epoch = 5630 loss = 0.242524 score = 0.960000\n",
      "epoch = 5640 loss = 0.242364 score = 0.960000\n",
      "epoch = 5650 loss = 0.242204 score = 0.960000\n",
      "epoch = 5660 loss = 0.242045 score = 0.960000\n",
      "epoch = 5670 loss = 0.241886 score = 0.960000\n",
      "epoch = 5680 loss = 0.241728 score = 0.960000\n",
      "epoch = 5690 loss = 0.241570 score = 0.960000\n",
      "epoch = 5700 loss = 0.241412 score = 0.960000\n",
      "epoch = 5710 loss = 0.241255 score = 0.960000\n",
      "epoch = 5720 loss = 0.241098 score = 0.960000\n",
      "epoch = 5730 loss = 0.240942 score = 0.960000\n",
      "epoch = 5740 loss = 0.240786 score = 0.960000\n",
      "epoch = 5750 loss = 0.240630 score = 0.960000\n",
      "epoch = 5760 loss = 0.240475 score = 0.960000\n",
      "epoch = 5770 loss = 0.240320 score = 0.960000\n",
      "epoch = 5780 loss = 0.240166 score = 0.960000\n",
      "epoch = 5790 loss = 0.240012 score = 0.960000\n",
      "epoch = 5800 loss = 0.239858 score = 0.960000\n",
      "epoch = 5810 loss = 0.239705 score = 0.960000\n",
      "epoch = 5820 loss = 0.239552 score = 0.960000\n",
      "epoch = 5830 loss = 0.239400 score = 0.960000\n",
      "epoch = 5840 loss = 0.239248 score = 0.960000\n",
      "epoch = 5850 loss = 0.239096 score = 0.960000\n",
      "epoch = 5860 loss = 0.238945 score = 0.960000\n",
      "epoch = 5870 loss = 0.238794 score = 0.960000\n",
      "epoch = 5880 loss = 0.238644 score = 0.960000\n",
      "epoch = 5890 loss = 0.238493 score = 0.960000\n",
      "epoch = 5900 loss = 0.238344 score = 0.960000\n",
      "epoch = 5910 loss = 0.238194 score = 0.960000\n",
      "epoch = 5920 loss = 0.238045 score = 0.960000\n",
      "epoch = 5930 loss = 0.237897 score = 0.960000\n",
      "epoch = 5940 loss = 0.237749 score = 0.960000\n",
      "epoch = 5950 loss = 0.237601 score = 0.960000\n",
      "epoch = 5960 loss = 0.237453 score = 0.960000\n",
      "epoch = 5970 loss = 0.237306 score = 0.960000\n",
      "epoch = 5980 loss = 0.237160 score = 0.960000\n",
      "epoch = 5990 loss = 0.237013 score = 0.960000\n",
      "epoch = 6000 loss = 0.236867 score = 0.960000\n",
      "epoch = 6010 loss = 0.236722 score = 0.960000\n",
      "epoch = 6020 loss = 0.236576 score = 0.960000\n",
      "epoch = 6030 loss = 0.236431 score = 0.960000\n",
      "epoch = 6040 loss = 0.236287 score = 0.960000\n",
      "epoch = 6050 loss = 0.236143 score = 0.960000\n",
      "epoch = 6060 loss = 0.235999 score = 0.960000\n",
      "epoch = 6070 loss = 0.235855 score = 0.960000\n",
      "epoch = 6080 loss = 0.235712 score = 0.960000\n",
      "epoch = 6090 loss = 0.235570 score = 0.960000\n",
      "epoch = 6100 loss = 0.235427 score = 0.960000\n",
      "epoch = 6110 loss = 0.235285 score = 0.960000\n",
      "epoch = 6120 loss = 0.235143 score = 0.960000\n",
      "epoch = 6130 loss = 0.235002 score = 0.960000\n",
      "epoch = 6140 loss = 0.234861 score = 0.960000\n",
      "epoch = 6150 loss = 0.234720 score = 0.960000\n",
      "epoch = 6160 loss = 0.234580 score = 0.960000\n",
      "epoch = 6170 loss = 0.234440 score = 0.960000\n",
      "epoch = 6180 loss = 0.234301 score = 0.960000\n",
      "epoch = 6190 loss = 0.234161 score = 0.960000\n",
      "epoch = 6200 loss = 0.234022 score = 0.960000\n",
      "epoch = 6210 loss = 0.233884 score = 0.960000\n",
      "epoch = 6220 loss = 0.233745 score = 0.960000\n",
      "epoch = 6230 loss = 0.233607 score = 0.960000\n",
      "epoch = 6240 loss = 0.233470 score = 0.960000\n",
      "epoch = 6250 loss = 0.233333 score = 0.960000\n",
      "epoch = 6260 loss = 0.233196 score = 0.960000\n",
      "epoch = 6270 loss = 0.233059 score = 0.960000\n",
      "epoch = 6280 loss = 0.232923 score = 0.960000\n",
      "epoch = 6290 loss = 0.232787 score = 0.960000\n",
      "epoch = 6300 loss = 0.232651 score = 0.960000\n",
      "epoch = 6310 loss = 0.232516 score = 0.960000\n",
      "epoch = 6320 loss = 0.232381 score = 0.960000\n",
      "epoch = 6330 loss = 0.232246 score = 0.960000\n",
      "epoch = 6340 loss = 0.232112 score = 0.960000\n",
      "epoch = 6350 loss = 0.231978 score = 0.960000\n",
      "epoch = 6360 loss = 0.231844 score = 0.960000\n",
      "epoch = 6370 loss = 0.231711 score = 0.960000\n",
      "epoch = 6380 loss = 0.231578 score = 0.960000\n",
      "epoch = 6390 loss = 0.231445 score = 0.960000\n",
      "epoch = 6400 loss = 0.231313 score = 0.960000\n",
      "epoch = 6410 loss = 0.231181 score = 0.960000\n",
      "epoch = 6420 loss = 0.231049 score = 0.960000\n",
      "epoch = 6430 loss = 0.230918 score = 0.960000\n",
      "epoch = 6440 loss = 0.230786 score = 0.960000\n",
      "epoch = 6450 loss = 0.230656 score = 0.960000\n",
      "epoch = 6460 loss = 0.230525 score = 0.960000\n",
      "epoch = 6470 loss = 0.230395 score = 0.960000\n",
      "epoch = 6480 loss = 0.230265 score = 0.960000\n",
      "epoch = 6490 loss = 0.230135 score = 0.960000\n",
      "epoch = 6500 loss = 0.230006 score = 0.960000\n",
      "epoch = 6510 loss = 0.229877 score = 0.960000\n",
      "epoch = 6520 loss = 0.229748 score = 0.960000\n",
      "epoch = 6530 loss = 0.229620 score = 0.960000\n",
      "epoch = 6540 loss = 0.229492 score = 0.960000\n",
      "epoch = 6550 loss = 0.229364 score = 0.960000\n",
      "epoch = 6560 loss = 0.229236 score = 0.960000\n",
      "epoch = 6570 loss = 0.229109 score = 0.960000\n",
      "epoch = 6580 loss = 0.228982 score = 0.960000\n",
      "epoch = 6590 loss = 0.228856 score = 0.960000\n",
      "epoch = 6600 loss = 0.228729 score = 0.960000\n",
      "epoch = 6610 loss = 0.228603 score = 0.960000\n",
      "epoch = 6620 loss = 0.228478 score = 0.960000\n",
      "epoch = 6630 loss = 0.228352 score = 0.960000\n",
      "epoch = 6640 loss = 0.228227 score = 0.960000\n",
      "epoch = 6650 loss = 0.228102 score = 0.960000\n",
      "epoch = 6660 loss = 0.227977 score = 0.960000\n",
      "epoch = 6670 loss = 0.227853 score = 0.960000\n",
      "epoch = 6680 loss = 0.227729 score = 0.960000\n",
      "epoch = 6690 loss = 0.227605 score = 0.960000\n",
      "epoch = 6700 loss = 0.227482 score = 0.960000\n",
      "epoch = 6710 loss = 0.227359 score = 0.960000\n",
      "epoch = 6720 loss = 0.227236 score = 0.960000\n",
      "epoch = 6730 loss = 0.227113 score = 0.960000\n",
      "epoch = 6740 loss = 0.226991 score = 0.960000\n",
      "epoch = 6750 loss = 0.226869 score = 0.960000\n",
      "epoch = 6760 loss = 0.226747 score = 0.960000\n",
      "epoch = 6770 loss = 0.226626 score = 0.960000\n",
      "epoch = 6780 loss = 0.226505 score = 0.960000\n",
      "epoch = 6790 loss = 0.226384 score = 0.960000\n",
      "epoch = 6800 loss = 0.226263 score = 0.960000\n",
      "epoch = 6810 loss = 0.226143 score = 0.960000\n",
      "epoch = 6820 loss = 0.226022 score = 0.960000\n",
      "epoch = 6830 loss = 0.225903 score = 0.960000\n",
      "epoch = 6840 loss = 0.225783 score = 0.960000\n",
      "epoch = 6850 loss = 0.225664 score = 0.960000\n",
      "epoch = 6860 loss = 0.225545 score = 0.960000\n",
      "epoch = 6870 loss = 0.225426 score = 0.960000\n",
      "epoch = 6880 loss = 0.225307 score = 0.960000\n",
      "epoch = 6890 loss = 0.225189 score = 0.960000\n",
      "epoch = 6900 loss = 0.225071 score = 0.960000\n",
      "epoch = 6910 loss = 0.224953 score = 0.960000\n",
      "epoch = 6920 loss = 0.224836 score = 0.960000\n",
      "epoch = 6930 loss = 0.224719 score = 0.960000\n",
      "epoch = 6940 loss = 0.224602 score = 0.960000\n",
      "epoch = 6950 loss = 0.224485 score = 0.960000\n",
      "epoch = 6960 loss = 0.224369 score = 0.960000\n",
      "epoch = 6970 loss = 0.224253 score = 0.960000\n",
      "epoch = 6980 loss = 0.224137 score = 0.960000\n",
      "epoch = 6990 loss = 0.224021 score = 0.960000\n",
      "epoch = 7000 loss = 0.223906 score = 0.960000\n",
      "epoch = 7010 loss = 0.223791 score = 0.960000\n",
      "epoch = 7020 loss = 0.223676 score = 0.960000\n",
      "epoch = 7030 loss = 0.223561 score = 0.960000\n",
      "epoch = 7040 loss = 0.223447 score = 0.960000\n",
      "epoch = 7050 loss = 0.223333 score = 0.960000\n",
      "epoch = 7060 loss = 0.223219 score = 0.960000\n",
      "epoch = 7070 loss = 0.223105 score = 0.960000\n",
      "epoch = 7080 loss = 0.222992 score = 0.960000\n",
      "epoch = 7090 loss = 0.222879 score = 0.960000\n",
      "epoch = 7100 loss = 0.222766 score = 0.960000\n",
      "epoch = 7110 loss = 0.222653 score = 0.960000\n",
      "epoch = 7120 loss = 0.222541 score = 0.960000\n",
      "epoch = 7130 loss = 0.222429 score = 0.960000\n",
      "epoch = 7140 loss = 0.222317 score = 0.960000\n",
      "epoch = 7150 loss = 0.222205 score = 0.960000\n",
      "epoch = 7160 loss = 0.222094 score = 0.960000\n",
      "epoch = 7170 loss = 0.221983 score = 0.960000\n",
      "epoch = 7180 loss = 0.221872 score = 0.960000\n",
      "epoch = 7190 loss = 0.221761 score = 0.960000\n",
      "epoch = 7200 loss = 0.221651 score = 0.960000\n",
      "epoch = 7210 loss = 0.221540 score = 0.960000\n",
      "epoch = 7220 loss = 0.221430 score = 0.960000\n",
      "epoch = 7230 loss = 0.221321 score = 0.960000\n",
      "epoch = 7240 loss = 0.221211 score = 0.960000\n",
      "epoch = 7250 loss = 0.221102 score = 0.960000\n",
      "epoch = 7260 loss = 0.220993 score = 0.960000\n",
      "epoch = 7270 loss = 0.220884 score = 0.960000\n",
      "epoch = 7280 loss = 0.220776 score = 0.960000\n",
      "epoch = 7290 loss = 0.220667 score = 0.960000\n",
      "epoch = 7300 loss = 0.220559 score = 0.960000\n",
      "epoch = 7310 loss = 0.220451 score = 0.960000\n",
      "epoch = 7320 loss = 0.220344 score = 0.960000\n",
      "epoch = 7330 loss = 0.220236 score = 0.960000\n",
      "epoch = 7340 loss = 0.220129 score = 0.960000\n",
      "epoch = 7350 loss = 0.220022 score = 0.960000\n",
      "epoch = 7360 loss = 0.219915 score = 0.960000\n",
      "epoch = 7370 loss = 0.219809 score = 0.960000\n",
      "epoch = 7380 loss = 0.219703 score = 0.960000\n",
      "epoch = 7390 loss = 0.219596 score = 0.960000\n",
      "epoch = 7400 loss = 0.219491 score = 0.960000\n",
      "epoch = 7410 loss = 0.219385 score = 0.960000\n",
      "epoch = 7420 loss = 0.219280 score = 0.960000\n",
      "epoch = 7430 loss = 0.219175 score = 0.960000\n",
      "epoch = 7440 loss = 0.219070 score = 0.960000\n",
      "epoch = 7450 loss = 0.218965 score = 0.960000\n",
      "epoch = 7460 loss = 0.218860 score = 0.960000\n",
      "epoch = 7470 loss = 0.218756 score = 0.960000\n",
      "epoch = 7480 loss = 0.218652 score = 0.960000\n",
      "epoch = 7490 loss = 0.218548 score = 0.960000\n",
      "epoch = 7500 loss = 0.218445 score = 0.960000\n",
      "epoch = 7510 loss = 0.218341 score = 0.960000\n",
      "epoch = 7520 loss = 0.218238 score = 0.960000\n",
      "epoch = 7530 loss = 0.218135 score = 0.960000\n",
      "epoch = 7540 loss = 0.218032 score = 0.960000\n",
      "epoch = 7550 loss = 0.217930 score = 0.960000\n",
      "epoch = 7560 loss = 0.217827 score = 0.960000\n",
      "epoch = 7570 loss = 0.217725 score = 0.960000\n",
      "epoch = 7580 loss = 0.217623 score = 0.960000\n",
      "epoch = 7590 loss = 0.217522 score = 0.960000\n",
      "epoch = 7600 loss = 0.217420 score = 0.960000\n",
      "epoch = 7610 loss = 0.217319 score = 0.960000\n",
      "epoch = 7620 loss = 0.217218 score = 0.960000\n",
      "epoch = 7630 loss = 0.217117 score = 0.960000\n",
      "epoch = 7640 loss = 0.217016 score = 0.960000\n",
      "epoch = 7650 loss = 0.216916 score = 0.960000\n",
      "epoch = 7660 loss = 0.216815 score = 0.960000\n",
      "epoch = 7670 loss = 0.216715 score = 0.960000\n",
      "epoch = 7680 loss = 0.216615 score = 0.960000\n",
      "epoch = 7690 loss = 0.216516 score = 0.960000\n",
      "epoch = 7700 loss = 0.216416 score = 0.960000\n",
      "epoch = 7710 loss = 0.216317 score = 0.960000\n",
      "epoch = 7720 loss = 0.216218 score = 0.960000\n",
      "epoch = 7730 loss = 0.216119 score = 0.960000\n",
      "epoch = 7740 loss = 0.216021 score = 0.960000\n",
      "epoch = 7750 loss = 0.215922 score = 0.960000\n",
      "epoch = 7760 loss = 0.215824 score = 0.960000\n",
      "epoch = 7770 loss = 0.215726 score = 0.960000\n",
      "epoch = 7780 loss = 0.215628 score = 0.960000\n",
      "epoch = 7790 loss = 0.215531 score = 0.960000\n",
      "epoch = 7800 loss = 0.215433 score = 0.960000\n",
      "epoch = 7810 loss = 0.215336 score = 0.960000\n",
      "epoch = 7820 loss = 0.215239 score = 0.960000\n",
      "epoch = 7830 loss = 0.215142 score = 0.960000\n",
      "epoch = 7840 loss = 0.215045 score = 0.960000\n",
      "epoch = 7850 loss = 0.214949 score = 0.960000\n",
      "epoch = 7860 loss = 0.214853 score = 0.960000\n",
      "epoch = 7870 loss = 0.214756 score = 0.960000\n",
      "epoch = 7880 loss = 0.214661 score = 0.960000\n",
      "epoch = 7890 loss = 0.214565 score = 0.960000\n",
      "epoch = 7900 loss = 0.214469 score = 0.960000\n",
      "epoch = 7910 loss = 0.214374 score = 0.960000\n",
      "epoch = 7920 loss = 0.214279 score = 0.960000\n",
      "epoch = 7930 loss = 0.214184 score = 0.960000\n",
      "epoch = 7940 loss = 0.214089 score = 0.960000\n",
      "epoch = 7950 loss = 0.213995 score = 0.960000\n",
      "epoch = 7960 loss = 0.213900 score = 0.960000\n",
      "epoch = 7970 loss = 0.213806 score = 0.960000\n",
      "epoch = 7980 loss = 0.213712 score = 0.960000\n",
      "epoch = 7990 loss = 0.213618 score = 0.960000\n",
      "epoch = 8000 loss = 0.213525 score = 0.960000\n",
      "epoch = 8010 loss = 0.213431 score = 0.960000\n",
      "epoch = 8020 loss = 0.213338 score = 0.960000\n",
      "epoch = 8030 loss = 0.213245 score = 0.960000\n",
      "epoch = 8040 loss = 0.213152 score = 0.960000\n",
      "epoch = 8050 loss = 0.213059 score = 0.960000\n",
      "epoch = 8060 loss = 0.212967 score = 0.960000\n",
      "epoch = 8070 loss = 0.212874 score = 0.960000\n",
      "epoch = 8080 loss = 0.212782 score = 0.960000\n",
      "epoch = 8090 loss = 0.212690 score = 0.960000\n",
      "epoch = 8100 loss = 0.212598 score = 0.960000\n",
      "epoch = 8110 loss = 0.212507 score = 0.960000\n",
      "epoch = 8120 loss = 0.212415 score = 0.960000\n",
      "epoch = 8130 loss = 0.212324 score = 0.960000\n",
      "epoch = 8140 loss = 0.212233 score = 0.960000\n",
      "epoch = 8150 loss = 0.212142 score = 0.960000\n",
      "epoch = 8160 loss = 0.212051 score = 0.960000\n",
      "epoch = 8170 loss = 0.211961 score = 0.960000\n",
      "epoch = 8180 loss = 0.211870 score = 0.960000\n",
      "epoch = 8190 loss = 0.211780 score = 0.960000\n",
      "epoch = 8200 loss = 0.211690 score = 0.960000\n",
      "epoch = 8210 loss = 0.211600 score = 0.960000\n",
      "epoch = 8220 loss = 0.211511 score = 0.960000\n",
      "epoch = 8230 loss = 0.211421 score = 0.960000\n",
      "epoch = 8240 loss = 0.211332 score = 0.960000\n",
      "epoch = 8250 loss = 0.211243 score = 0.960000\n",
      "epoch = 8260 loss = 0.211154 score = 0.960000\n",
      "epoch = 8270 loss = 0.211065 score = 0.960000\n",
      "epoch = 8280 loss = 0.210976 score = 0.960000\n",
      "epoch = 8290 loss = 0.210888 score = 0.960000\n",
      "epoch = 8300 loss = 0.210799 score = 0.960000\n",
      "epoch = 8310 loss = 0.210711 score = 0.960000\n",
      "epoch = 8320 loss = 0.210623 score = 0.960000\n",
      "epoch = 8330 loss = 0.210535 score = 0.960000\n",
      "epoch = 8340 loss = 0.210448 score = 0.960000\n",
      "epoch = 8350 loss = 0.210360 score = 0.960000\n",
      "epoch = 8360 loss = 0.210273 score = 0.960000\n",
      "epoch = 8370 loss = 0.210186 score = 0.960000\n",
      "epoch = 8380 loss = 0.210099 score = 0.960000\n",
      "epoch = 8390 loss = 0.210012 score = 0.960000\n",
      "epoch = 8400 loss = 0.209925 score = 0.960000\n",
      "epoch = 8410 loss = 0.209839 score = 0.960000\n",
      "epoch = 8420 loss = 0.209752 score = 0.960000\n",
      "epoch = 8430 loss = 0.209666 score = 0.960000\n",
      "epoch = 8440 loss = 0.209580 score = 0.960000\n",
      "epoch = 8450 loss = 0.209494 score = 0.960000\n",
      "epoch = 8460 loss = 0.209409 score = 0.960000\n",
      "epoch = 8470 loss = 0.209323 score = 0.960000\n",
      "epoch = 8480 loss = 0.209238 score = 0.960000\n",
      "epoch = 8490 loss = 0.209153 score = 0.960000\n",
      "epoch = 8500 loss = 0.209067 score = 0.960000\n",
      "epoch = 8510 loss = 0.208983 score = 0.960000\n",
      "epoch = 8520 loss = 0.208898 score = 0.960000\n",
      "epoch = 8530 loss = 0.208813 score = 0.960000\n",
      "epoch = 8540 loss = 0.208729 score = 0.960000\n",
      "epoch = 8550 loss = 0.208645 score = 0.960000\n",
      "epoch = 8560 loss = 0.208560 score = 0.960000\n",
      "epoch = 8570 loss = 0.208477 score = 0.960000\n",
      "epoch = 8580 loss = 0.208393 score = 0.960000\n",
      "epoch = 8590 loss = 0.208309 score = 0.960000\n",
      "epoch = 8600 loss = 0.208226 score = 0.960000\n",
      "epoch = 8610 loss = 0.208142 score = 0.960000\n",
      "epoch = 8620 loss = 0.208059 score = 0.960000\n",
      "epoch = 8630 loss = 0.207976 score = 0.960000\n",
      "epoch = 8640 loss = 0.207893 score = 0.960000\n",
      "epoch = 8650 loss = 0.207811 score = 0.960000\n",
      "epoch = 8660 loss = 0.207728 score = 0.960000\n",
      "epoch = 8670 loss = 0.207646 score = 0.960000\n",
      "epoch = 8680 loss = 0.207563 score = 0.960000\n",
      "epoch = 8690 loss = 0.207481 score = 0.960000\n",
      "epoch = 8700 loss = 0.207399 score = 0.960000\n",
      "epoch = 8710 loss = 0.207317 score = 0.960000\n",
      "epoch = 8720 loss = 0.207236 score = 0.960000\n",
      "epoch = 8730 loss = 0.207154 score = 0.960000\n",
      "epoch = 8740 loss = 0.207073 score = 0.960000\n",
      "epoch = 8750 loss = 0.206992 score = 0.960000\n",
      "epoch = 8760 loss = 0.206911 score = 0.960000\n",
      "epoch = 8770 loss = 0.206830 score = 0.960000\n",
      "epoch = 8780 loss = 0.206749 score = 0.960000\n",
      "epoch = 8790 loss = 0.206668 score = 0.960000\n",
      "epoch = 8800 loss = 0.206588 score = 0.960000\n",
      "epoch = 8810 loss = 0.206508 score = 0.960000\n",
      "epoch = 8820 loss = 0.206427 score = 0.960000\n",
      "epoch = 8830 loss = 0.206347 score = 0.960000\n",
      "epoch = 8840 loss = 0.206267 score = 0.960000\n",
      "epoch = 8850 loss = 0.206188 score = 0.960000\n",
      "epoch = 8860 loss = 0.206108 score = 0.960000\n",
      "epoch = 8870 loss = 0.206029 score = 0.960000\n",
      "epoch = 8880 loss = 0.205949 score = 0.960000\n",
      "epoch = 8890 loss = 0.205870 score = 0.960000\n",
      "epoch = 8900 loss = 0.205791 score = 0.960000\n",
      "epoch = 8910 loss = 0.205712 score = 0.960000\n",
      "epoch = 8920 loss = 0.205634 score = 0.960000\n",
      "epoch = 8930 loss = 0.205555 score = 0.960000\n",
      "epoch = 8940 loss = 0.205476 score = 0.960000\n",
      "epoch = 8950 loss = 0.205398 score = 0.960000\n",
      "epoch = 8960 loss = 0.205320 score = 0.960000\n",
      "epoch = 8970 loss = 0.205242 score = 0.960000\n",
      "epoch = 8980 loss = 0.205164 score = 0.960000\n",
      "epoch = 8990 loss = 0.205086 score = 0.960000\n",
      "epoch = 9000 loss = 0.205009 score = 0.960000\n",
      "epoch = 9010 loss = 0.204931 score = 0.960000\n",
      "epoch = 9020 loss = 0.204854 score = 0.960000\n",
      "epoch = 9030 loss = 0.204776 score = 0.960000\n",
      "epoch = 9040 loss = 0.204699 score = 0.960000\n",
      "epoch = 9050 loss = 0.204622 score = 0.960000\n",
      "epoch = 9060 loss = 0.204546 score = 0.960000\n",
      "epoch = 9070 loss = 0.204469 score = 0.960000\n",
      "epoch = 9080 loss = 0.204392 score = 0.960000\n",
      "epoch = 9090 loss = 0.204316 score = 0.960000\n",
      "epoch = 9100 loss = 0.204240 score = 0.960000\n",
      "epoch = 9110 loss = 0.204164 score = 0.960000\n",
      "epoch = 9120 loss = 0.204088 score = 0.960000\n",
      "epoch = 9130 loss = 0.204012 score = 0.960000\n",
      "epoch = 9140 loss = 0.203936 score = 0.960000\n",
      "epoch = 9150 loss = 0.203860 score = 0.960000\n",
      "epoch = 9160 loss = 0.203785 score = 0.960000\n",
      "epoch = 9170 loss = 0.203710 score = 0.960000\n",
      "epoch = 9180 loss = 0.203634 score = 0.960000\n",
      "epoch = 9190 loss = 0.203559 score = 0.960000\n",
      "epoch = 9200 loss = 0.203484 score = 0.960000\n",
      "epoch = 9210 loss = 0.203410 score = 0.960000\n",
      "epoch = 9220 loss = 0.203335 score = 0.960000\n",
      "epoch = 9230 loss = 0.203260 score = 0.960000\n",
      "epoch = 9240 loss = 0.203186 score = 0.960000\n",
      "epoch = 9250 loss = 0.203112 score = 0.960000\n",
      "epoch = 9260 loss = 0.203037 score = 0.960000\n",
      "epoch = 9270 loss = 0.202963 score = 0.960000\n",
      "epoch = 9280 loss = 0.202889 score = 0.960000\n",
      "epoch = 9290 loss = 0.202816 score = 0.960000\n",
      "epoch = 9300 loss = 0.202742 score = 0.960000\n",
      "epoch = 9310 loss = 0.202668 score = 0.960000\n",
      "epoch = 9320 loss = 0.202595 score = 0.960000\n",
      "epoch = 9330 loss = 0.202522 score = 0.960000\n",
      "epoch = 9340 loss = 0.202449 score = 0.960000\n",
      "epoch = 9350 loss = 0.202376 score = 0.960000\n",
      "epoch = 9360 loss = 0.202303 score = 0.960000\n",
      "epoch = 9370 loss = 0.202230 score = 0.960000\n",
      "epoch = 9380 loss = 0.202157 score = 0.960000\n",
      "epoch = 9390 loss = 0.202085 score = 0.960000\n",
      "epoch = 9400 loss = 0.202012 score = 0.960000\n",
      "epoch = 9410 loss = 0.201940 score = 0.960000\n",
      "epoch = 9420 loss = 0.201868 score = 0.960000\n",
      "epoch = 9430 loss = 0.201796 score = 0.960000\n",
      "epoch = 9440 loss = 0.201724 score = 0.960000\n",
      "epoch = 9450 loss = 0.201652 score = 0.960000\n",
      "epoch = 9460 loss = 0.201581 score = 0.960000\n",
      "epoch = 9470 loss = 0.201509 score = 0.960000\n",
      "epoch = 9480 loss = 0.201438 score = 0.960000\n",
      "epoch = 9490 loss = 0.201366 score = 0.960000\n",
      "epoch = 9500 loss = 0.201295 score = 0.960000\n",
      "epoch = 9510 loss = 0.201224 score = 0.960000\n",
      "epoch = 9520 loss = 0.201153 score = 0.960000\n",
      "epoch = 9530 loss = 0.201082 score = 0.960000\n",
      "epoch = 9540 loss = 0.201012 score = 0.960000\n",
      "epoch = 9550 loss = 0.200941 score = 0.960000\n",
      "epoch = 9560 loss = 0.200870 score = 0.960000\n",
      "epoch = 9570 loss = 0.200800 score = 0.960000\n",
      "epoch = 9580 loss = 0.200730 score = 0.960000\n",
      "epoch = 9590 loss = 0.200660 score = 0.960000\n",
      "epoch = 9600 loss = 0.200590 score = 0.960000\n",
      "epoch = 9610 loss = 0.200520 score = 0.960000\n",
      "epoch = 9620 loss = 0.200450 score = 0.960000\n",
      "epoch = 9630 loss = 0.200381 score = 0.960000\n",
      "epoch = 9640 loss = 0.200311 score = 0.960000\n",
      "epoch = 9650 loss = 0.200242 score = 0.960000\n",
      "epoch = 9660 loss = 0.200172 score = 0.960000\n",
      "epoch = 9670 loss = 0.200103 score = 0.960000\n",
      "epoch = 9680 loss = 0.200034 score = 0.960000\n",
      "epoch = 9690 loss = 0.199965 score = 0.960000\n",
      "epoch = 9700 loss = 0.199896 score = 0.960000\n",
      "epoch = 9710 loss = 0.199828 score = 0.960000\n",
      "epoch = 9720 loss = 0.199759 score = 0.960000\n",
      "epoch = 9730 loss = 0.199690 score = 0.960000\n",
      "epoch = 9740 loss = 0.199622 score = 0.960000\n",
      "epoch = 9750 loss = 0.199554 score = 0.960000\n",
      "epoch = 9760 loss = 0.199486 score = 0.960000\n",
      "epoch = 9770 loss = 0.199418 score = 0.960000\n",
      "epoch = 9780 loss = 0.199350 score = 0.960000\n",
      "epoch = 9790 loss = 0.199282 score = 0.960000\n",
      "epoch = 9800 loss = 0.199214 score = 0.960000\n",
      "epoch = 9810 loss = 0.199147 score = 0.960000\n",
      "epoch = 9820 loss = 0.199079 score = 0.960000\n",
      "epoch = 9830 loss = 0.199012 score = 0.960000\n",
      "epoch = 9840 loss = 0.198944 score = 0.960000\n",
      "epoch = 9850 loss = 0.198877 score = 0.960000\n",
      "epoch = 9860 loss = 0.198810 score = 0.960000\n",
      "epoch = 9870 loss = 0.198743 score = 0.960000\n",
      "epoch = 9880 loss = 0.198676 score = 0.960000\n",
      "epoch = 9890 loss = 0.198610 score = 0.960000\n",
      "epoch = 9900 loss = 0.198543 score = 0.960000\n",
      "epoch = 9910 loss = 0.198477 score = 0.960000\n",
      "epoch = 9920 loss = 0.198410 score = 0.960000\n",
      "epoch = 9930 loss = 0.198344 score = 0.960000\n",
      "epoch = 9940 loss = 0.198278 score = 0.960000\n",
      "epoch = 9950 loss = 0.198212 score = 0.960000\n",
      "epoch = 9960 loss = 0.198146 score = 0.960000\n",
      "epoch = 9970 loss = 0.198080 score = 0.960000\n",
      "epoch = 9980 loss = 0.198014 score = 0.960000\n",
      "epoch = 9990 loss = 0.197948 score = 0.960000\n"
     ]
    }
   ],
   "source": [
    "#  メイン処理\n",
    "for k in range(iters):\n",
    "\n",
    "    # 予測値の計算 (9.7.1)　(9.7.2)\n",
    "    yp = pred(x, W)\n",
    "\n",
    "    # 誤差の計算 (9.7.4)\n",
    "    yd = yp - yt\n",
    "\n",
    "    # 重みの更新 (9.7.5)\n",
    "    W = W - alpha * (x.T @ yd) / M\n",
    "\n",
    "    if (k % 10 == 0):\n",
    "        loss, score = evaluate(x_test, y_test, y_test_one, W)\n",
    "        history = np.vstack((history,\n",
    "            np.array([k, loss, score])))\n",
    "        print(\"epoch = %d loss = %f score = %f\"\n",
    "            % (k, loss, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9oIwwg8XlxT"
   },
   "source": [
    "### 結果確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA4BEyWCXlxU"
   },
   "outputs": [],
   "source": [
    "#損失関数値と精度の確認\n",
    "print('初期状態: 損失関数:%f 精度:%f'\n",
    "    % (history[0,1], history[0,2]))\n",
    "print('最終状態: 損失関数:%f 精度:%f'\n",
    "    % (history[-1,1], history[-1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPKmEaNOXlxU"
   },
   "outputs": [],
   "source": [
    "# 学習曲線の表示 (損失関数)\n",
    "plt.plot(history[:,0], history[:,1])\n",
    "plt.grid()\n",
    "plt.ylim(0,1.2)\n",
    "plt.xlabel('iter', fontsize=14)\n",
    "plt.ylabel('loss', fontsize=14)\n",
    "plt.title('iter vs loss', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdXJN5gCXlxU"
   },
   "outputs": [],
   "source": [
    "# 学習曲線の表示 (精度)\n",
    "plt.plot(history[:,0], history[:,2])\n",
    "plt.ylim(0,1)\n",
    "plt.grid()\n",
    "plt.xlabel('iter', fontsize=14)\n",
    "plt.ylabel('accuracy', fontsize=14)\n",
    "plt.title('iter vs accuracy', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chH4gf1oXlxU"
   },
   "outputs": [],
   "source": [
    "# 3次元表示\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "x1 = np.linspace(4, 8.5, 100)\n",
    "x2 = np.linspace(0.5, 7.5, 100)\n",
    "xx1, xx2 = np.meshgrid(x1, x2)\n",
    "xxx = np.array([np.ones(xx1.ravel().shape),\n",
    "    xx1.ravel(), xx2.ravel()]).T\n",
    "pp = pred(xxx, W)\n",
    "c0 = pp[:,0].reshape(xx1.shape)\n",
    "c1 = pp[:,1].reshape(xx1.shape)\n",
    "c2 = pp[:,2].reshape(xx1.shape)\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1, projection='3d')\n",
    "ax.plot_surface(xx1, xx2, c0, color='lightblue',\n",
    "    edgecolor='black', rstride=10, cstride=10, alpha=0.7)\n",
    "ax.plot_surface(xx1, xx2, c1, color='blue',\n",
    "    edgecolor='black', rstride=10, cstride=10, alpha=0.7)\n",
    "ax.plot_surface(xx1, xx2, c2, color='lightgrey',\n",
    "    edgecolor='black', rstride=10, cstride=10, alpha=0.7)\n",
    "ax.scatter(x_t0[:,0], x_t0[:,1], 1, s=50, alpha=1, marker='+', c='k')\n",
    "ax.scatter(x_t1[:,0], x_t1[:,1], 1, s=30, alpha=1, marker='o', c='k')\n",
    "ax.scatter(x_t2[:,0], x_t2[:,1], 1, s=50, alpha=1, marker='x', c='k')\n",
    "ax.set_xlim(4,8.5)\n",
    "ax.set_ylim(0.5,7.5)\n",
    "ax.view_init(elev=40, azim=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHPNORc3XlxV"
   },
   "outputs": [],
   "source": [
    "# 評価\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# テストデータで予測値の計算\n",
    "yp_test_one = pred(x_test, W)\n",
    "yp_test = np.argmax(yp_test_one, axis=1)\n",
    "\n",
    "#  精度の計算\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_test, yp_test)\n",
    "print('accuracy: %f' % score)\n",
    "\n",
    "# 混同行列の表示\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "print(classification_report(y_test, yp_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHWsDGhiXlxV"
   },
   "source": [
    "# 入力変数を4次元に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHrgbhqgXlxW"
   },
   "outputs": [],
   "source": [
    "# ダミー変数を追加\n",
    "x_all2 = np.insert(x_org, 0, 1.0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nkbHNGxXlxW"
   },
   "outputs": [],
   "source": [
    "# 学習データ、検証データに分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train2, x_test2, y_train, y_test,\\\n",
    "y_train_one, y_test_one = train_test_split(\n",
    "    x_all2, y_org, y_all_one, train_size=75,\n",
    "    test_size=75, random_state=123)\n",
    "print(x_train2.shape, x_test2.shape,\n",
    "    y_train.shape, y_test.shape,\n",
    "    y_train_one.shape, y_test_one.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IB1AKt1VXlxW"
   },
   "outputs": [],
   "source": [
    "print('入力データ(x)')\n",
    "print(x_train2[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4u10E8QgXlxW"
   },
   "outputs": [],
   "source": [
    "# 学習対象の選択\n",
    "x, yt, x_test  = x_train2, y_train_one, x_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK2a_rfQXlxW"
   },
   "outputs": [],
   "source": [
    "# 初期化処理\n",
    "\n",
    "# 標本数\n",
    "M  = x.shape[0]\n",
    "# 入力次元数(ダミー変数を含む\n",
    "D = x.shape[1]\n",
    "# 分類先クラス数\n",
    "N = yt.shape[1]\n",
    "\n",
    "# 繰り返し回数\n",
    "iters = 10000\n",
    "\n",
    "# 学習率\n",
    "alpha = 0.01\n",
    "\n",
    "# 重み行列の初期設定(すべて1)\n",
    "W = np.ones((D, N))\n",
    "\n",
    "# 評価結果記録用\n",
    "history = np.zeros((0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sM_OgoWXlxW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  メイン処理(４次元版)\n",
    "for k in range(iters):\n",
    "\n",
    "    # 予測値の計算 (9.7.1)　(9.7.2)\n",
    "    yp = pred(x, W)\n",
    "\n",
    "    # 誤差の計算 (9.7.4)\n",
    "    yd = yp - yt\n",
    "\n",
    "    # 重みの更新 (9.7.5)\n",
    "    W = W - alpha * (x.T @ yd) / M\n",
    "\n",
    "    if (k % 10 == 0):\n",
    "        loss, score = evaluate(x_test, y_test, y_test_one, W)\n",
    "        history = np.vstack((history, np.array([k, loss, score])))\n",
    "        print(\"epoch = %d loss = %f score = %f\" % (k, loss, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSlmcXaFXlxX"
   },
   "outputs": [],
   "source": [
    "print(history.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8NiJYCfXlxX"
   },
   "outputs": [],
   "source": [
    "#損失関数値と精度の確認\n",
    "print('初期状態: 損失関数:%f 精度:%f'\n",
    "    % (history[0,1], history[0,2]))\n",
    "print('最終状態: 損失関数:%f 精度:%f'\n",
    "    % (history[-1,1], history[-1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiCMeNTeXlxX"
   },
   "outputs": [],
   "source": [
    "# 学習曲線の表示 (損失関数)\n",
    "plt.plot(history[:,0], history[:,1])\n",
    "plt.ylim(0,1.2)\n",
    "plt.grid()\n",
    "plt.xlabel('iter', fontsize=14)\n",
    "plt.ylabel('loss', fontsize=14)\n",
    "plt.title('iter vs loss', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVPRk8yRXlxX"
   },
   "outputs": [],
   "source": [
    "# 学習曲線の表示 (精度)\n",
    "plt.plot(history[:,0], history[:,2])\n",
    "plt.ylim(0,1)\n",
    "plt.grid()\n",
    "plt.xlabel('iter', fontsize=14)\n",
    "plt.ylabel('accuracy', fontsize=14)\n",
    "plt.title('iter vs accuracy', fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
